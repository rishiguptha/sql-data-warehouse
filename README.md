# Modern Data Warehouse With DuckDB, Python and Docker
A modern data warehouse built with **DuckDB, Python, and Docker** using the Medallion Architecture (Bronze â†’ Silver â†’ Gold). Consolidates sales data from two source systems (CRM + ERP) into a star schema optimized for analytical reporting.

> Based on the [Data with Baara SQL Data Warehouse Project](https://www.youtube.com/playlist?list=PLNcg_FV9n7qaUWeyUkPfiVtMbKlrfMqA8), modified to use a cloud-native, serverless stack instead of SQL Server.

---

## Tech Stack

| Tool | Version | Purpose |
|------|---------|---------| 
| **DuckDB** | â‰¥ 1.4.4 | In-process analytical database (replaces SQL Server) |
| **Python** | â‰¥ 3.12 | Pipeline orchestration and ingestion scripts |
| **Docker** | â€” | Containerized, reproducible environment |
| **uv** | â€” | Fast Python package management |

---

## Architecture

![Architecture Diagram](docs/architecture.png)

| Layer | Role | Load Strategy |
|-------|------|---------------|
| **Bronze** | Raw ingestion â€” data as-is from source CSVs | `CREATE OR REPLACE TABLE` (full replace) |
| **Silver** | Cleaned & standardized â€” nulls, types, dedup, business rules | `DROP + CREATE` then `TRUNCATE + INSERT` (full reload) |
| **Gold** | Star schema â€” query-ready materialized tables for BI | `DROP + CREATE` (full rebuild) |

### Data Flow

![Data Flow Diagram](docs/sql-datawarehouse-dataflow.png)

---

## Project Structure

```
sql-data-warehouse/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ warehouse.duckdb              # DuckDB database file (auto-created, gitignored)
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ source_crm/                   # CRM source CSVs
â”‚   â”‚   â”œâ”€â”€ cust_info.csv
â”‚   â”‚   â”œâ”€â”€ prd_info.csv
â”‚   â”‚   â””â”€â”€ sales_details.csv
â”‚   â””â”€â”€ source_erp/                   # ERP source CSVs
â”‚       â”œâ”€â”€ CUST_AZ12.csv
â”‚       â”œâ”€â”€ LOC_A101.csv
â”‚       â””â”€â”€ PX_CAT_G1V2.csv
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.png              # Medallion architecture diagram
â”‚   â”œâ”€â”€ data_model.png                # Gold layer star schema (ER diagram)
â”‚   â”œâ”€â”€ Integration-model.png         # CRM + ERP source integration model
â”‚   â”œâ”€â”€ sql-datawarehouse-dataflow.png # End-to-end data flow diagram
â”‚   â””â”€â”€ data_catalog.md               # Gold layer data dictionary
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ load_bronze.py            # Ingest CSVs â†’ bronze schema
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ load_silver.sql           # Clean + standardize â†’ silver schema
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â””â”€â”€ load_gold.sql             # Star schema â†’ gold schema
â”‚   â”œâ”€â”€ init_db.py                    # Create bronze/silver/gold schemas
â”‚   â””â”€â”€ run_pipeline.py               # Orchestrate full pipeline end-to-end
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ quality_checks_bronze.sql     # Nulls, duplicates, whitespace, date validity
â”‚   â”œâ”€â”€ quality_checks_silver.sql     # Referential integrity, standardization
â”‚   â””â”€â”€ quality_checks_gold.sql       # Surrogate key uniqueness, FK integrity
â”œâ”€â”€ Dockerfile                        # Container image definition
â”œâ”€â”€ docker-compose.yml                # Service + volume mount config
â”œâ”€â”€ pyproject.toml                    # Python project + dependencies (uv)
â””â”€â”€ README.md
```

---

## Data Sources

Two simulated source systems, 6 CSV files total:

![Integration Model](docs/Integration-model.png)

| Schema | Table | Rows (approx.) | Description |
|--------|-------|----------------|-------------|
| CRM | `cust_info` | ~18K | Customer profiles â€” name, gender, marital status |
| CRM | `prd_info` | ~400 | Product catalog â€” name, cost, line, date ranges |
| CRM | `sales_details` | ~60K | Sales transactions â€” orders, quantities, prices |
| ERP | `CUST_AZ12` | ~18K | Customer demographics â€” birthdate, gender |
| ERP | `LOC_A101` | ~18K | Customer location â€” country |
| ERP | `PX_CAT_G1V2` | ~37 | Product categories and subcategories |

---

## Gold Layer â€” Star Schema

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  dim_customers   â”‚
                    â”‚  customer_key PK â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ (FK)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_products   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚                  â”‚
â”‚  product_key PK  â”‚  (FK)   â””â”€â”€â”€â”€â–º fact_sales â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Table | Columns | Grain | Description |
|-------|---------|-------|-------------|
| `gold.dim_customers` | 10 | One row per customer | CRM + ERP merged, gender master from CRM |
| `gold.dim_products` | 11 | One row per active product | CRM + ERP enriched, historical versions excluded |
| `gold.fact_sales` | 9 | One row per order line | Corrected sales metrics, surrogate FK lookups |

![Data Model](docs/data_model.png)

> ðŸ“– Full column-level documentation: [`docs/data_catalog.md`](docs/data_catalog.md)

---

## Documentation

| File | Description |
|------|-------------|
| [`docs/architecture.png`](docs/architecture.png) | Medallion architecture overview (Bronze â†’ Silver â†’ Gold) |
| [`docs/sql-datawarehouse-dataflow.png`](docs/sql-datawarehouse-dataflow.png) | End-to-end data flow from source CSVs through all three layers |
| [`docs/Integration-model.png`](docs/Integration-model.png) | CRM + ERP source system integration model showing how tables are joined |
| [`docs/data_model.png`](docs/data_model.png) | Gold layer star schema ER diagram (dim_customers, dim_products, fact_sales) |
| [`docs/data_catalog.md`](docs/data_catalog.md) | Gold layer data dictionary â€” entity relationships, column definitions, business rules, and sample queries |

---

## Naming Conventions

| Layer | Pattern | Example |
|-------|---------|---------| 
| Bronze | `<source>_<entity>` | `crm_cust_info`, `erp_loc_a101` |
| Silver | `<source>_<entity>` (cleaned) | `crm_cust_info`, `erp_cust_az12` |
| Gold Dimensions | `dim_<entity>` | `dim_customers`, `dim_products` |
| Gold Facts | `fact_<entity>` | `fact_sales` |
| Surrogate keys | `<entity>_key` | `customer_key`, `product_key` |
| Audit columns | `dwh_<name>` | `dwh_load_date`, `dwh_create_date` |

---

## How to Run

### Option A â€” Local (uv)

#### Prerequisites

```bash
pip install uv
uv sync
```

#### Full pipeline (one command)

```bash
uv run scripts/run_pipeline.py
```

This runs all three layers in sequence: init â†’ bronze â†’ silver â†’ gold.

#### Step-by-step (manual)

```bash
# Step 1 â€” Initialize schemas
uv run scripts/init_db.py

# Step 2 â€” Load bronze layer
uv run scripts/bronze/load_bronze.py

# Step 3 â€” Load silver layer
duckdb data/warehouse.duckdb < scripts/silver/load_silver.sql

# Step 4 â€” Load gold layer
duckdb data/warehouse.duckdb < scripts/gold/load_gold.sql
```

#### Run quality checks

```bash
# After bronze load
duckdb data/warehouse.duckdb < tests/quality_checks_bronze.sql

# After silver load
duckdb data/warehouse.duckdb < tests/quality_checks_silver.sql

# After gold load
duckdb data/warehouse.duckdb < tests/quality_checks_gold.sql
```

---

### Option B â€” Docker

```bash
# Build and run (mounts ./data so the .duckdb file persists on your host)
docker compose up

# Rebuild image after code changes
docker compose up --build
```

The container runs `uv run scripts/run_pipeline.py` and exits after the pipeline completes. The `warehouse.duckdb` file is written to `./data/` on your host via the volume mount.

---

## Pipeline Status

| Layer | Status | Script |
|-------|--------|--------|
| Database Init | âœ… Complete | `scripts/init_db.py` |
| Bronze | âœ… Complete | `scripts/bronze/load_bronze.py` |
| Silver | âœ… Complete | `scripts/silver/load_silver.sql` |
| Gold | âœ… Complete | `scripts/gold/load_gold.sql` |
| Quality Checks | âœ… Complete | `tests/quality_checks_*.sql` |
| Data Catalog | âœ… Complete | `docs/data_catalog.md` |
| Orchestration | âœ… Complete | `scripts/run_pipeline.py` |
| Docker | âœ… Complete | `Dockerfile`, `docker-compose.yml` |

---

## Key Design Decisions

**DuckDB over SQL Server** â€” File-based, no server setup. Runs in-process with Python. Anyone can clone and run in 60 seconds with zero infrastructure.

**Materialized tables over views** â€” Gold layer writes data to disk. Downstream BI tools query pre-computed results, not live SQL re-executions. Production-grade pattern.

**Full load (Truncate & Insert)** â€” All 3 layers use full load. Bronze uses `CREATE OR REPLACE TABLE`; Silver uses `DROP + CREATE` for DDL then `TRUNCATE + INSERT` for data; Gold uses `DROP + CREATE`. Source data is small and historical â€” no need for incremental merge at this scale.

**Idempotent scripts** â€” Every script is safe to re-run with no side effects:
- Bronze: `CREATE OR REPLACE TABLE` atomically replaces the table.
- Silver: `DROP TABLE IF EXISTS` + `CREATE` re-establishes the schema, then `TRUNCATE + INSERT` reloads all rows.
- Gold: `DROP TABLE IF EXISTS` + `CREATE TABLE AS SELECT` fully rebuilds each table.

**Surrogate keys are not stable across reloads** â€” `customer_key` and `product_key` are generated with `ROW_NUMBER()`. They are reassigned on every full rebuild. Always join on surrogate keys within the warehouse; do not persist or cache these integers in external systems.

**CRM as master system for gender** â€” When CRM and ERP conflict on gender, CRM value is used. ERP is the fallback only when CRM returns `n/a`.

**Audit column on every table** â€” `dwh_load_date` (bronze) and `dwh_create_date` (silver) track when each record entered the warehouse. Required for debugging and lineage tracing.

**Silver cleans, Gold joins** â€” Data corrections (dedup, type casting, business rule standardization) happen in Silver. Gold is purely about joining and reshaping for analytical consumption.
