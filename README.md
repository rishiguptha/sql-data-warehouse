# SQL Data Warehouse â€” Portfolio Project

A modern data warehouse built with **DuckDB, Python, and Docker** using the Medallion Architecture (Bronze â†’ Silver â†’ Gold). Consolidates sales data from two source systems (CRM + ERP) into a star schema optimized for analytical reporting.

> Based on the [Data with Baara SQL Data Warehouse Project](https://www.youtube.com/playlist?list=PLNcg_FV9n7qaUWeyUkPfiVtMbKlrfMqA8), modified to use a cloud-native, server-less stack instead of SQL Server.

---

## Tech Stack

| Tool | Purpose |
|------|---------|
| **DuckDB** | In-process analytical database (replaces SQL Server) |
| **Python** | Pipeline orchestration and ingestion scripts |
| **Docker** | Containerized, reproducible environment |
| **uv** | Fast Python package management |

---

## Architecture

```
Source CSVs (CRM + ERP)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bronze Layer   â”‚  Raw ingestion â€” as-is from source, full load
â”‚  (Ingest)       â”‚  + dwh_load_date audit column
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Silver Layer   â”‚  Clean & standardize â€” nulls, types, dedup
â”‚  (Clean)        â”‚  Data normalization + derived columns
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Gold Layer    â”‚  Star schema â€” dim_customers, dim_products,
â”‚  (Business)     â”‚  fact_sales â€” materialized tables, query-ready
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Architecture diagram: [`docs/architecture.png`](docs/architecture.png)

---

## Project Structure

```
sql-data-warehouse/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ warehouse.duckdb          # DuckDB database file
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ source_crm/               # CRM source CSVs
â”‚   â”‚   â”œâ”€â”€ cust_info.csv
â”‚   â”‚   â”œâ”€â”€ prd_info.csv
â”‚   â”‚   â””â”€â”€ sales_details.csv
â”‚   â””â”€â”€ source_erp/               # ERP source CSVs
â”‚       â”œâ”€â”€ CUST_AZ12.csv
â”‚       â”œâ”€â”€ LOC_A101.csv
â”‚       â””â”€â”€ PX_CAT_G1V2.csv
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.png
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ load_bronze.py        # Ingest CSVs â†’ bronze schema
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ load_silver.sql       # Clean + standardize â†’ silver schema
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â””â”€â”€ load_gold.sql         # Star schema â†’ gold schema
â”‚   â”œâ”€â”€ init_db.py                # Create schemas
â”‚   â””â”€â”€ run_pipeline.py           # Orchestrate full pipeline
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ quality_checks_bronze.sql # Row counts, nulls, duplicates
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## Data Sources

Two simulated source systems, 6 CSV files total:

| Schema | Table | Description |
|--------|-------|-------------|
| CRM | `cust_info` | Customer profiles â€” name, gender, marital status |
| CRM | `prd_info` | Product catalog â€” name, cost, line, dates |
| CRM | `sales_details` | Sales transactions â€” orders, quantities, prices |
| ERP | `CUST_AZ12` | Customer demographics â€” birthdate, gender |
| ERP | `LOC_A101` | Customer location â€” country |
| ERP | `PX_CAT_G1V2` | Product categories and subcategories |

---

## Naming Conventions

| Layer | Pattern | Example |
|-------|---------|---------|
| Bronze | `<source>_<entity>` | `crm_cust_info`, `erp_loc_a101` |
| Silver | `<source>_<entity>` | `crm_cust_info` (cleaned) |
| Gold Dimensions | `dim_<entity>` | `dim_customers`, `dim_products` |
| Gold Facts | `fact_<entity>` | `fact_sales` |
| Surrogate keys | `<entity>_key` | `customer_key`, `product_key` |
| Audit columns | `dwh_<name>` | `dwh_load_date`, `dwh_source` |

---

## How to Run

### Option 1: Python (local)

```bash
# Install dependencies
pip install uv
uv sync

# Initialize database
uv run scripts/init_db.py

# Run full pipeline
uv run scripts/run_pipeline.py
```

### Option 2: Docker

```bash
docker-compose up
```

---

## Pipeline Progress

| Layer | Status | Script |
|-------|--------|--------|
| Bronze | âœ… Complete | `scripts/bronze/load_bronze.py` |
| Silver | ğŸ”„ In Progress | `scripts/silver/load_silver.sql` |
| Gold | â¬œ Not Started | `scripts/gold/load_gold.sql` |
| Orchestration | â¬œ Not Started | `scripts/run_pipeline.py` |
| Docker | â¬œ Not Started | `Dockerfile`, `docker-compose.yml` |

---

## Key Design Decisions

**DuckDB over SQL Server** â€” File-based, no server setup. Runs in-process with Python. Recruiter can clone and run in 60 seconds with zero infrastructure.

**Materialized tables over views** â€” Gold layer writes data to disk. Downstream BI tools query pre-computed results, not live SQL re-executions. Production-grade pattern.

**Full load (Truncate & Insert)** â€” All 3 layers use full load. Source data is small and historical; no need for incremental merge at this scale.

**Audit column on every table** â€” `dwh_load_date` tracks when each record entered the warehouse. Required for debugging and lineage tracing.