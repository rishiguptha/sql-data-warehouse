# SQL Data Warehouse â€” Portfolio Project

A modern data warehouse built with **DuckDB, Python, and Docker** using the Medallion Architecture (Bronze â†’ Silver â†’ Gold). Consolidates sales data from two source systems (CRM + ERP) into a star schema optimized for analytical reporting.

> Based on the [Data with Baara SQL Data Warehouse Project](https://www.youtube.com/playlist?list=PLNcg_FV9n7qaUWeyUkPfiVtMbKlrfMqA8), modified to use a cloud-native, serverless stack instead of SQL Server.

---

## Tech Stack

| Tool | Version | Purpose |
|------|---------|---------|
| **DuckDB** | â‰¥ 1.4.4 | In-process analytical database (replaces SQL Server) |
| **Python** | â‰¥ 3.12 | Pipeline orchestration and ingestion scripts |
| **pandas** | â‰¥ 3.0.1 | Data manipulation (available for future transforms) |
| **Docker** | â€” | Containerized, reproducible environment *(in progress)* |
| **uv** | â€” | Fast Python package management |

---

## Architecture

![Architecture Diagram](docs/architecture.png)

| Layer | Role | Load Strategy |
|-------|------|---------------|
| **Bronze** | Raw ingestion â€” data as-is from source CSVs | `CREATE OR REPLACE TABLE` (full replace) |
| **Silver** | Cleaned & standardized â€” nulls, types, dedup, business rules | `TRUNCATE + INSERT` (full reload) |
| **Gold** | Star schema â€” query-ready materialized tables for BI | `DROP + CREATE` (full rebuild) |

### Data Flow

![Data Flow Diagram](docs/sql-datawarehouse-dataflow.png)

---

## Project Structure

```
sql-data-warehouse/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ warehouse.duckdb              # DuckDB database file (auto-created)
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ source_crm/                   # CRM source CSVs
â”‚   â”‚   â”œâ”€â”€ cust_info.csv
â”‚   â”‚   â”œâ”€â”€ prd_info.csv
â”‚   â”‚   â””â”€â”€ sales_details.csv
â”‚   â””â”€â”€ source_erp/                   # ERP source CSVs
â”‚       â”œâ”€â”€ CUST_AZ12.csv
â”‚       â”œâ”€â”€ LOC_A101.csv
â”‚       â””â”€â”€ PX_CAT_G1V2.csv
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.png              # Medallion architecture diagram
â”‚   â”œâ”€â”€ Data Model.png                # Gold layer star schema (ER diagram)
â”‚   â”œâ”€â”€ Integration-model.png         # CRM + ERP source integration model
â”‚   â”œâ”€â”€ sql-datawarehouse-dataflow.png # End-to-end data flow diagram
â”‚   â””â”€â”€ data_catalog.md              # Gold layer data dictionary
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ load_bronze.py           # Ingest CSVs â†’ bronze schema
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ load_silver.sql          # Clean + standardize â†’ silver schema
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â””â”€â”€ load_gold.sql            # Star schema â†’ gold schema
â”‚   â”œâ”€â”€ init_db.py                   # Create bronze/silver/gold schemas
â”‚   â””â”€â”€ run_pipeline.py              # Orchestrate full pipeline â¬œ in progress
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ quality_checks_bronze.sql    # Row counts, nulls, duplicates in bronze
â”‚   â”œâ”€â”€ quality_checks_silver.sql    # Referential integrity, standardization in silver
â”‚   â””â”€â”€ quality_checks_gold.sql      # FK integrity checks in gold
â”œâ”€â”€ Dockerfile                        # â¬œ in progress
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml                    # Python project + dependencies (uv)
â””â”€â”€ README.md
```

---

## Data Sources

Two simulated source systems, 6 CSV files total:

![Integration Model](docs/Integration-model.png)

| Schema | Table | Rows (approx.) | Description |
|--------|-------|----------------|-------------|
| CRM | `cust_info` | ~18K | Customer profiles â€” name, gender, marital status |
| CRM | `prd_info` | ~400 | Product catalog â€” name, cost, line, date ranges |
| CRM | `sales_details` | ~60K | Sales transactions â€” orders, quantities, prices |
| ERP | `CUST_AZ12` | ~18K | Customer demographics â€” birthdate, gender |
| ERP | `LOC_A101` | ~18K | Customer location â€” country |
| ERP | `PX_CAT_G1V2` | ~37 | Product categories and subcategories |

---

## Gold Layer â€” Star Schema

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  dim_customers   â”‚
                    â”‚  customer_key PK â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ (FK)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_products   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚                  â”‚
â”‚  product_key PK  â”‚  (FK)   â””â”€â”€â”€â”€â–º fact_sales â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Table | Columns | Grain | Description |
|-------|---------|-------|-------------|
| `gold.dim_customers` | 10 | One row per customer | CRM + ERP merged, gender master from CRM |
| `gold.dim_products` | 11 | One row per active product | CRM + ERP enriched, historical versions excluded |
| `gold.fact_sales` | 9 | One row per order line | Corrected sales metrics, surrogate FK lookups |

![Data Model](docs/data_model.png)

> ðŸ“– Full column-level documentation: [`docs/data_catalog.md`](docs/data_catalog.md)

---

## Documentation

| File | Description |
|------|-------------|
| [`docs/architecture.png`](docs/architecture.png) | Medallion architecture overview (Bronze â†’ Silver â†’ Gold) |
| [`docs/sql-datawarehouse-dataflow.png`](docs/sql-datawarehouse-dataflow.png) | End-to-end data flow from source CSVs through all three layers |
| [`docs/Integration-model.png`](docs/Integration-model.png) | CRM + ERP source system integration model showing how tables are joined |
| [`docs/Data Model.png`](docs/Data%20Model.png) | Gold layer star schema ER diagram (dim_customers, dim_products, fact_sales) |
| [`docs/data_catalog.md`](docs/data_catalog.md) | Gold layer data dictionary â€” entity relationships, column definitions, business rules, and sample queries |

---


## Naming Conventions

| Layer | Pattern | Example |
|-------|---------|---------|
| Bronze | `<source>_<entity>` | `crm_cust_info`, `erp_loc_a101` |
| Silver | `<source>_<entity>` (cleaned) | `crm_cust_info`, `erp_cust_az12` |
| Gold Dimensions | `dim_<entity>` | `dim_customers`, `dim_products` |
| Gold Facts | `fact_<entity>` | `fact_sales` |
| Surrogate keys | `<entity>_key` | `customer_key`, `product_key` |
| Audit columns | `dwh_<name>` | `dwh_load_date`, `dwh_create_date` |

---

## How to Run

### Prerequisites

```bash
pip install uv
uv sync
```

### Step 1 â€” Initialize the database

```bash
uv run scripts/init_db.py
```

Creates the `data/warehouse.duckdb` file with `bronze`, `silver`, and `gold` schemas.

### Step 2 â€” Load bronze layer

```bash
uv run scripts/bronze/load_bronze.py
```

Ingests all 6 CSV files into the `bronze` schema as-is, with a `dwh_load_date` audit column.

### Step 3 â€” Load silver layer

```bash
duckdb data/warehouse.duckdb < scripts/silver/load_silver.sql
```

Cleans, casts, deduplicates, and standardizes data into the `silver` schema.

### Step 4 â€” Load gold layer

```bash
duckdb data/warehouse.duckdb < scripts/gold/load_gold.sql
```

Builds `dim_customers`, `dim_products`, and `fact_sales` in the `gold` schema.

### Run quality checks

```bash
# After bronze load
duckdb data/warehouse.duckdb < tests/quality_checks_bronze.sql

# After silver load
duckdb data/warehouse.duckdb < tests/quality_checks_silver.sql

# After gold load
duckdb data/warehouse.duckdb < tests/quality_checks_gold.sql
```

---

## Pipeline Progress

| Layer | Status | Script |
|-------|--------|--------|
| Database Init | âœ… Complete | `scripts/init_db.py` |
| Bronze | âœ… Complete | `scripts/bronze/load_bronze.py` |
| Silver | âœ… Complete | `scripts/silver/load_silver.sql` |
| Gold | âœ… Complete | `scripts/gold/load_gold.sql` |
| Quality Checks | âœ… Complete | `tests/quality_checks_*.sql` |
| Data Catalog | âœ… Complete | `docs/data_catalog.md` |
| Orchestration | â¬œ Not Started | `scripts/run_pipeline.py` |
| Docker | â¬œ Not Started | `Dockerfile`, `docker-compose.yml` |

---

## Key Design Decisions

**DuckDB over SQL Server** â€” File-based, no server setup. Runs in-process with Python. Anyone can clone and run in 60 seconds with zero infrastructure.

**Materialized tables over views** â€” Gold layer writes data to disk. Downstream BI tools query pre-computed results, not live SQL re-executions. Production-grade pattern.

**Full load (Truncate & Insert)** â€” All 3 layers use full load. Silver uses `TRUNCATE + INSERT` for safe reloads without schema changes; Bronze uses `CREATE OR REPLACE`. Source data is small and historical â€” no need for incremental merge at this scale.

**Idempotent scripts** â€” Every script is safe to re-run. Bronze uses `CREATE OR REPLACE TABLE`, Silver uses `DROP + CREATE` for DDL then `TRUNCATE + INSERT` for data, Gold uses `DROP + CREATE`.

**CRM as master system for gender** â€” When CRM and ERP conflict on gender, CRM value is used. ERP is the fallback only when CRM returns `n/a`.

**Audit column on every table** â€” `dwh_load_date` (bronze) and `dwh_create_date` (silver) track when each record entered the warehouse. Required for debugging and lineage tracing.

**Silver cleans, Gold joins** â€” Data corrections (dedup, type casting, business rule standardization) happen in Silver. Gold is purely about joining and reshaping for analytical consumption.